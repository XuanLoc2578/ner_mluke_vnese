Index: run_train.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/bin/bash\n\npython ner_mluke/run_train.py \\\n    --model_name_or_path studio-ousia/luke-large-finetuned-conll-2003 \\\n    --cache_dir mounts/model/mluke \\\n    --train_dataset_file mounts/dataset/written_train_git.txt \\\n    --dev_dataset_file mounts/dataset/written_dev_git.txt \\\n    --test_dataset_file mounts/dataset/written_test_git.txt \\\n    --max_seq_length=256 \\\n    --epochs=1 \\\n    --batch_size=4 \\\n    --num_workers=2 \\\n    --lr=0.00005\n#    --output_dir \\\n#    --read_file_dir home/nxloc/huggingface_model/data/tmp_data \\\n#    --write_file_dir home/nxloc/huggingface_model/data/written_tmp_data \\
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/run_train.sh b/run_train.sh
--- a/run_train.sh	(revision 6aa157e31bccc84b36a2002de3786b1ac9299f4c)
+++ b/run_train.sh	(date 1669015496388)
@@ -1,8 +1,10 @@
 #!/bin/bash
 
 python ner_mluke/run_train.py \
-    --model_name_or_path studio-ousia/luke-large-finetuned-conll-2003 \
+    --pretrained_model_name_or_path studio-ousia/luke-large-finetuned-conll-2003 \
     --cache_dir mounts/model/mluke \
+    --read_file_dir mounts/dataset/dev_git.csv \
+    --write_file_dir mounts/dataset/written_dev_git.txt \
     --train_dataset_file mounts/dataset/written_train_git.txt \
     --dev_dataset_file mounts/dataset/written_dev_git.txt \
     --test_dataset_file mounts/dataset/written_test_git.txt \
@@ -12,5 +14,3 @@
     --num_workers=2 \
     --lr=0.00005
 #    --output_dir \
-#    --read_file_dir home/nxloc/huggingface_model/data/tmp_data \
-#    --write_file_dir home/nxloc/huggingface_model/data/written_tmp_data \
\ No newline at end of file
Index: ner_mluke/mydataset.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import tqdm\nimport unicodedata\n\nimport torch\nfrom torch.utils.data import Dataset as Dataset, DataLoader as DataLoader, TensorDataset\nimport transformers\nfrom transformers import AutoConfig, LukeForEntitySpanClassification, LukeTokenizer\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nimport training_arguments, feature_compress\nfrom feature_compress import FeatureExtracted\n\n\nclass ReadAndWriteFile:\n    def read_file(self, read_file_dir):\n        read_file_dir = training_arguments.ModelArguments.read_file_dir\n        with open(read_file_dir, 'r+') as f:\n            lines = f.readlines()\n\n        text = '-DOCSTART-\\t-X-\\t-X-\\tO\\n\\n'\n        count = 0\n        for i in range(3, len(lines) - 1):\n            if lines[i] == '\\n':\n                count += 1\n            if count == 2:\n                lines.insert(i + 1, text)\n                count = 0\n        lines.append('\\n\\n' + text)\n\n        return lines\n\n    def write_file(self, lines, write_file_dir):\n        write_file_dir = training_arguments.ModelArguments.write_file_dir\n        lines = self.read_file(training_arguments.ModelArguments.read_file_dir)\n\n        if write_file_dir is not None:\n            with open(write_file_dir, 'w') as f:\n                for line in lines:\n                    f.write(f\"{line}\")\n\n\nclass DataProcessor:\n\n    def gen_function(self, dataset_file):\n        model_name_or_path = training_arguments.ModelArguments.model_name_or_path\n        max_seq_length = training_arguments.DataTrainingArguments.max_seq_length\n        batch_size = training_arguments.DataTrainingArguments.batch_size\n        num_workers = training_arguments.DataTrainingArguments.num_workers\n\n\n        tokenizer = self.huggingface_download(model_name_or_path=model_name_or_path)\n        documents = self.load_documents(dataset_file=dataset_file)\n        examples = self.load_examples(max_seq_length=max_seq_length, documents=documents, tokenizer=tokenizer)\n        model_config, custom_label2id = self.custom_model_config(model_name_or_path=model_name_or_path, documents=documents)\n        final_tag_list = self.create_tag_list(documents=documents, examples=examples)\n\n        label_id_list = self.create_label_id(\n            examples=examples,\n            tag_list=final_tag_list,\n            custom_label2id=custom_label2id,\n            tokenizer=tokenizer,\n            max_seq_length=max_seq_length\n        )\n        params_list = self.create_list_params(examples=examples, tokenizer=tokenizer, max_seq_length=max_seq_length)\n        dataloader = self.create_dataset_and_dataloader(params_list=params_list,\n                                                        label_id_tensor=label_id_list,\n                                                        batch_size=batch_size,\n                                                        num_workers=num_workers\n                                                        )\n        return model_config, dataloader\n\n\n    def huggingface_download(self, model_name_or_path):\n        cache_dir = training_arguments.ModelArguments.cache_dir\n        print('model_name_or_path: {} \\n cache_dir: {}'.format(model_name_or_path, cache_dir))\n        tokenizer = LukeTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n\n        return tokenizer\n\n    def load_documents(self, dataset_file) -> list[dict]:\n        line = '-DOCSTART-  -X- -X-\tO'\n        documents = []\n        words = []\n        labels = []\n        sentence_boundaries = []\n        with open(dataset_file) as f:\n            for line in f:\n                line = line.rstrip()\n                if line.startswith(\"-DOCSTART-\"):\n                    if words:\n                        documents.append(dict(\n                            words=words,\n                            labels=labels,\n                            sentence_boundaries=sentence_boundaries\n                        ))\n                        words = []\n                        labels = []\n                        sentence_boundaries = []\n                    continue\n\n                if not line:  # chỗ này cho '\\n\\n'\n                    if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n                        sentence_boundaries.append(len(words))\n                else:  # các line bình thường\n                    items = line.split(\"\\t\")\n                    words.append(items[0])\n                    labels.append(items[-1])\n\n        if words:\n            documents.append(dict(\n                words=words,\n                labels=labels,\n                sentence_boundaries=sentence_boundaries\n            ))\n\n        return documents\n\n    def load_examples(self, max_seq_length, documents, tokenizer) -> list[dict]:\n        examples = []\n        # max_token_length = 510\n        max_mention_length = 30\n\n        for ind, document in enumerate(tqdm(documents)):\n            words = document[\"words\"]\n            subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n            total_subword_length = sum(subword_lengths)\n            sentence_boundaries = document[\"sentence_boundaries\"]\n\n            for i in range(len(sentence_boundaries) - 1):\n                sentence_start, sentence_end = sentence_boundaries[i:i + 2]\n                if total_subword_length <= max_seq_length:\n                    # if the total sequence length of the document is shorter than the\n                    # maximum token length, we simply use all words to build the sequence\n                    context_start = 0\n                    context_end = len(words)\n\n                else:\n                    # if the total sequence length is longer than the maximum length, we add\n                    # the surrounding words of the target sentence　to the sequence until it\n                    # reaches the maximum length\n                    context_start = sentence_start\n                    context_end = sentence_end\n                    cur_length = sum(subword_lengths[context_start:context_end])\n\n                    while True:\n                        if context_start > 0:\n                            if cur_length + subword_lengths[context_start - 1] <= max_seq_length:\n                                cur_length += subword_lengths[context_start - 1]\n                                context_start -= 1\n                            else:\n                                break\n                        if context_end < len(words):\n                            if cur_length + subword_lengths[context_end] <= max_seq_length:\n                                cur_length += subword_lengths[context_end]\n                                context_end += 1\n                            else:\n                                break\n\n                text = \"\"\n                for word in words[context_start:sentence_start]:\n                    if word[0] == \"'\" or (len(word) == 1 and self.is_punctuation(word)):\n                        text = text.rstrip()\n                    text += word\n                    text += \" \"\n\n                sentence_words = words[sentence_start:sentence_end]\n                sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n\n                word_start_char_positions = []\n                word_end_char_positions = []\n                for word in sentence_words:\n                    if word[0] == \"'\" or (len(word) == 1 and self.is_punctuation(word)):\n                        text = text.rstrip()\n                    word_start_char_positions.append(len(text))\n                    text += word\n                    word_end_char_positions.append(len(text))\n                    text += \" \"\n\n                for word in words[sentence_end:context_end]:\n                    if word[0] == \"'\" or (len(word) == 1 and self.is_punctuation(word)):\n                        text = text.rstrip()\n                    text += word\n                    text += \" \"\n                text = text.rstrip()\n\n                entity_spans = []\n                original_word_spans = []\n                for word_start in range(len(sentence_words)):\n                    for word_end in range(word_start, len(sentence_words)):\n                        if sum(sentence_subword_lengths[word_start:word_end]) <= max_mention_length:\n                            entity_spans.append(\n                                (word_start_char_positions[word_start], word_end_char_positions[word_end])\n                            )\n                            original_word_spans.append(\n                                (word_start, word_end + 1)\n                            )\n\n                examples.append(dict(\n                    text=text,\n                    words=sentence_words,\n                    entity_spans=entity_spans,\n                    original_word_spans=original_word_spans,\n                ))\n\n        return examples\n\n    def is_punctuation(self, char):\n        cp = ord(char)\n        if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n            return True\n        cat = unicodedata.category(char)\n        if cat.startswith(\"P\"):\n            return True\n        return False\n\n#####\n\n    def custom_model_config(self, model_name_or_path, documents):\n        model_config = AutoConfig.from_pretrained(model_name_or_path)\n\n        unique_tag = []\n        for i in range(len(documents)):\n            for j in range(len(documents[i]['labels'])):\n                if documents[i]['labels'][j] not in unique_tag:\n                    unique_tag.append(documents[i]['labels'][j])\n\n        custom_id2label = {i: label for i, label in enumerate(unique_tag)}\n        custom_label2id = {label: i for i, label in enumerate(unique_tag)}\n\n        model_config.id2label = custom_id2label\n        model_config.label2id = custom_label2id\n\n        return model_config, custom_label2id\n\n    def create_tag_list(self, documents, examples) -> list[list]:\n        final_word_list = []\n        for i in range(len(examples)):\n            final_word_list.append(examples[i]['words'])\n\n        list_all_tag = []\n        for i in range(len(documents)):\n            for j in range(len(documents[i]['labels'])):\n                list_all_tag.append(documents[i]['labels'][j])\n\n        final_tag_list = []\n        ind = 0\n        for i in range(len(final_word_list)):\n            tmp_list = list_all_tag[ind:ind + len(final_word_list[i])]\n            final_tag_list.append(tmp_list)\n            ind += len(final_word_list[i])\n\n        return final_tag_list\n\n    def create_list_params(self, examples, max_seq_length, tokenizer) -> object:\n        list_input_ids, list_attention_mask, list_entity_ids, list_entity_position_ids, list_entity_attention_mask = [], [], [], [], []\n\n        for i in range(len(examples)):\n            source_encoding = tokenizer(\n                text=' '.join(examples[i][\"Word\"]),\n                entity_spans=examples[i][\"Span\"],\n                max_length=max_seq_length,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                add_special_tokens=True,\n                # return_tensors=\"pt\"\n            )\n\n            for j in range(len(source_encoding['entity_position_ids'])):\n                if len(source_encoding['entity_position_ids'][j]) > 30:\n                    source_encoding['entity_position_ids'][j] = source_encoding['entity_position_ids'][j][:30]\n\n            list_input_ids.append(source_encoding[\"input_ids\"])\n            list_attention_mask.append(source_encoding[\"attention_mask\"])\n            list_entity_ids.append(source_encoding[\"entity_ids\"])\n            list_entity_position_ids.append(source_encoding[\"entity_position_ids\"])\n            list_entity_attention_mask.append(source_encoding[\"entity_attention_mask\"])\n\n        list_input_ids_tensor = torch.tensor([f for f in list_input_ids])\n        list_attention_mask_tensor = torch.tensor([f for f in list_attention_mask])\n        list_entity_ids_tensor = torch.tensor([f for f in list_entity_ids])\n        list_entity_position_ids_tensor = torch.tensor([f for f in list_entity_position_ids])\n        list_entity_attention_mask_tensor = torch.tensor([f for f in list_entity_attention_mask])\n\n        list_params = FeatureExtracted(\n            list_input_ids_tensor,\n            list_attention_mask_tensor,\n            list_entity_ids_tensor,\n            list_entity_position_ids_tensor,\n            list_entity_attention_mask_tensor\n        )\n        return list_params\n\n    def create_label_id(self, examples, tag_list, custom_label2id, max_seq_length, tokenizer) -> list[list]:\n        label_id = []\n        final_word_list = []\n        final_tag_list = []\n        for i in range(len(examples)):\n            final_word_list.append(examples[i]['words'])\n            final_tag_list.append(examples[i]['entity_spans'])\n        custom_label2id = custom_label2id\n        label_id_tensor_list = []\n\n        for i in range(len(final_word_list[:len(final_word_list)])):\n            tmp_label_id = []\n\n            for j in range(len(final_word_list[i])):\n                token = tokenizer.tokenize(final_word_list[i][j])\n                for t in range(len(token)):\n                    if t == 0:\n                        tmp_label_id.append(custom_label2id[final_tag_list[i][j]])\n                    else:\n                        tmp_label_id.append(0)\n\n            if len(tmp_label_id) > max_seq_length - 2:\n                tmp_label_id = tmp_label_id[:max_seq_length - 2]\n\n            tmp_label_id.insert(1, 0)\n            tmp_label_id.append(1)\n\n            while len(tmp_label_id) < max_seq_length:\n                tmp_label_id.append(0)\n\n            label_id.append(tmp_label_id)\n\n        label_id_tensor = torch.tensor([f for f in label_id])\n        return label_id_tensor\n\n    def create_dataset_and_dataloader(self, params_list, label_id_tensor, batch_size, num_workers):\n        input_ids_tensor, attention_mask_tensor, entity_ids_tensor, entity_position_ids_tensor, entity_attention_mask_tensor = params_list\n\n        dataset = TensorDataset(input_ids_tensor,\n                                attention_mask_tensor,\n                                entity_ids_tensor,\n                                entity_position_ids_tensor,\n                                entity_attention_mask_tensor,\n                                label_id_tensor\n                                )\n        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n\n        return dataloader\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ner_mluke/mydataset.py b/ner_mluke/mydataset.py
--- a/ner_mluke/mydataset.py	(revision 6aa157e31bccc84b36a2002de3786b1ac9299f4c)
+++ b/ner_mluke/mydataset.py	(date 1669015200180)
@@ -6,11 +6,11 @@
 import transformers
 from transformers import AutoConfig, LukeForEntitySpanClassification, LukeTokenizer
 
-import pandas as pd
-import numpy as np
-import matplotlib.pyplot as plt
-import sklearn
-from sklearn.model_selection import train_test_split
+# import pandas as pd
+# import numpy as np
+# import matplotlib.pyplot as plt
+# import sklearn
+# from sklearn.model_selection import train_test_split
 
 import training_arguments, feature_compress
 from feature_compress import FeatureExtracted
@@ -47,16 +47,15 @@
 class DataProcessor:
 
     def gen_function(self, dataset_file):
-        model_name_or_path = training_arguments.ModelArguments.model_name_or_path
+        pretrained_model_name_or_path = training_arguments.ModelArguments.pretrained_model_name_or_path
         max_seq_length = training_arguments.DataTrainingArguments.max_seq_length
         batch_size = training_arguments.DataTrainingArguments.batch_size
         num_workers = training_arguments.DataTrainingArguments.num_workers
 
-
-        tokenizer = self.huggingface_download(model_name_or_path=model_name_or_path)
+        tokenizer = self.huggingface_download(pretrained_model_name_or_path=pretrained_model_name_or_path)
         documents = self.load_documents(dataset_file=dataset_file)
         examples = self.load_examples(max_seq_length=max_seq_length, documents=documents, tokenizer=tokenizer)
-        model_config, custom_label2id = self.custom_model_config(model_name_or_path=model_name_or_path, documents=documents)
+        model_config, custom_label2id = self.custom_model_config(pretrained_model_name_or_path=pretrained_model_name_or_path, documents=documents)
         final_tag_list = self.create_tag_list(documents=documents, examples=examples)
 
         label_id_list = self.create_label_id(
@@ -75,14 +74,17 @@
         return model_config, dataloader
 
 
-    def huggingface_download(self, model_name_or_path):
+    def huggingface_download(self, pretrained_model_name_or_path):
         cache_dir = training_arguments.ModelArguments.cache_dir
-        print('model_name_or_path: {} \n cache_dir: {}'.format(model_name_or_path, cache_dir))
-        tokenizer = LukeTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)
+        access_token = input('Huggingface token: ')
+        print('pretrained_model_name_or_path: {} \n cache_dir: {}'.format(pretrained_model_name_or_path, cache_dir))
+        tokenizer = LukeTokenizer.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path,
+                                                  cache_dir=cache_dir,
+                                                  use_auth_token=access_token)
 
         return tokenizer
 
-    def load_documents(self, dataset_file) -> list[dict]:
+    def load_documents(self, dataset_file):
         line = '-DOCSTART-  -X- -X-	O'
         documents = []
         words = []
@@ -120,7 +122,7 @@
 
         return documents
 
-    def load_examples(self, max_seq_length, documents, tokenizer) -> list[dict]:
+    def load_examples(self, max_seq_length, documents, tokenizer):
         examples = []
         # max_token_length = 510
         max_mention_length = 30
@@ -220,8 +222,8 @@
 
 #####
 
-    def custom_model_config(self, model_name_or_path, documents):
-        model_config = AutoConfig.from_pretrained(model_name_or_path)
+    def custom_model_config(self, pretrained_model_name_or_path, documents):
+        model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)
 
         unique_tag = []
         for i in range(len(documents)):
@@ -237,7 +239,7 @@
 
         return model_config, custom_label2id
 
-    def create_tag_list(self, documents, examples) -> list[list]:
+    def create_tag_list(self, documents, examples):
         final_word_list = []
         for i in range(len(examples)):
             final_word_list.append(examples[i]['words'])
@@ -256,7 +258,7 @@
 
         return final_tag_list
 
-    def create_list_params(self, examples, max_seq_length, tokenizer) -> object:
+    def create_list_params(self, examples, max_seq_length, tokenizer):
         list_input_ids, list_attention_mask, list_entity_ids, list_entity_position_ids, list_entity_attention_mask = [], [], [], [], []
 
         for i in range(len(examples)):
@@ -296,7 +298,7 @@
         )
         return list_params
 
-    def create_label_id(self, examples, tag_list, custom_label2id, max_seq_length, tokenizer) -> list[list]:
+    def create_label_id(self, examples, tag_list, custom_label2id, max_seq_length, tokenizer):
         label_id = []
         final_word_list = []
         final_tag_list = []
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>python==3.7\ntorch==1.12.1\ntransformers==4.24.0\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 6aa157e31bccc84b36a2002de3786b1ac9299f4c)
+++ b/requirements.txt	(date 1669005050407)
@@ -1,3 +1,3 @@
-python==3.7
 torch==1.12.1
 transformers==4.24.0
+
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"527d37a0-072e-4f01-9fa9-ec3554a11b33\" name=\"Changes\" comment=\"\" />\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectId\" id=\"2HpzbGuokEZkufZjUtaFj5Ejas5\" />\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\"><![CDATA[{\n  \"keyToString\": {\n    \"RunOnceActivity.OpenProjectViewOnStart\": \"true\",\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\n    \"WebServerToolWindowFactoryState\": \"false\",\n    \"last_opened_file_path\": \"/home/nxloc/PycharmProjects_clone/ner_mluke_vnese\",\n    \"nodejs_package_manager_path\": \"npm\",\n    \"settings.editor.selected.configurable\": \"com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable\"\n  }\n}]]></component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"527d37a0-072e-4f01-9fa9-ec3554a11b33\" name=\"Changes\" comment=\"\" />\n      <created>1668996882461</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1668996882461</updated>\n      <workItem from=\"1668996885286\" duration=\"36000\" />\n      <workItem from=\"1668997053090\" duration=\"1164000\" />\n    </task>\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"3\" />\n  </component>\n  <component name=\"Vcs.Log.Tabs.Properties\">\n    <option name=\"TAB_STATES\">\n      <map>\n        <entry key=\"MAIN\">\n          <value>\n            <State />\n          </value>\n        </entry>\n      </map>\n    </option>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 6aa157e31bccc84b36a2002de3786b1ac9299f4c)
+++ b/.idea/workspace.xml	(date 1669019611672)
@@ -1,7 +1,12 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="ChangeListManager">
-    <list default="true" id="527d37a0-072e-4f01-9fa9-ec3554a11b33" name="Changes" comment="" />
+    <list default="true" id="527d37a0-072e-4f01-9fa9-ec3554a11b33" name="Changes" comment="Initial commit">
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/ner_mluke/mydataset.py" beforeDir="false" afterPath="$PROJECT_DIR$/ner_mluke/mydataset.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/requirements.txt" beforeDir="false" afterPath="$PROJECT_DIR$/requirements.txt" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/run_train.sh" beforeDir="false" afterPath="$PROJECT_DIR$/run_train.sh" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
@@ -15,6 +20,16 @@
     </option>
   </component>
   <component name="Git.Settings">
+    <option name="PREVIOUS_COMMIT_AUTHORS">
+      <list>
+        <option value="Xuan Loc Nguyen &lt;xuanloc2578.ptit@gmail.com&gt;" />
+      </list>
+    </option>
+    <option name="RECENT_BRANCH_BY_REPOSITORY">
+      <map>
+        <entry key="$PROJECT_DIR$" value="6aa157e31bccc84b36a2002de3786b1ac9299f4c" />
+      </map>
+    </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
   <component name="MarkdownSettingsMigration">
@@ -26,16 +41,40 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "RunOnceActivity.OpenProjectViewOnStart": "true",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "WebServerToolWindowFactoryState": "false",
-    "last_opened_file_path": "/home/nxloc/PycharmProjects_clone/ner_mluke_vnese",
-    "nodejs_package_manager_path": "npm",
-    "settings.editor.selected.configurable": "com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable"
-  }
-}]]></component>
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,
+    &quot;last_opened_file_path&quot;: &quot;/home/nxloc/PycharmProjects_clone/ner_mluke_vnese&quot;,
+    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;
+  }
+}</component>
+  <component name="RunManager">
+    <configuration name="run_train.sh" type="ShConfigurationType" temporary="true">
+      <option name="SCRIPT_TEXT" value="" />
+      <option name="INDEPENDENT_SCRIPT_PATH" value="true" />
+      <option name="SCRIPT_PATH" value="$PROJECT_DIR$/run_train.sh" />
+      <option name="SCRIPT_OPTIONS" value="" />
+      <option name="INDEPENDENT_SCRIPT_WORKING_DIRECTORY" value="true" />
+      <option name="SCRIPT_WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="INDEPENDENT_INTERPRETER_PATH" value="true" />
+      <option name="INTERPRETER_PATH" value="/bin/bash" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="EXECUTE_IN_TERMINAL" value="true" />
+      <option name="EXECUTE_SCRIPT_FILE" value="true" />
+      <envs />
+      <method v="2" />
+    </configuration>
+    <recent_temporary>
+      <list>
+        <item itemvalue="Shell Script.run_train.sh" />
+        <item itemvalue="Shell Script.run_train.sh" />
+        <item itemvalue="Shell Script.run_train.sh" />
+      </list>
+    </recent_temporary>
+  </component>
   <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
   <component name="TaskManager">
     <task active="true" id="Default" summary="Default task">
@@ -45,8 +84,31 @@
       <option name="presentableId" value="Default" />
       <updated>1668996882461</updated>
       <workItem from="1668996885286" duration="36000" />
-      <workItem from="1668997053090" duration="1164000" />
+      <workItem from="1668997053090" duration="9008000" />
+      <workItem from="1669019292903" duration="319000" />
     </task>
+    <task id="LOCAL-00001" summary="Initial commit">
+      <created>1668998330918</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1668998330918</updated>
+    </task>
+    <task id="LOCAL-00002" summary="Initial commit">
+      <created>1668998450966</created>
+      <option name="number" value="00002" />
+      <option name="presentableId" value="LOCAL-00002" />
+      <option name="project" value="LOCAL" />
+      <updated>1668998450966</updated>
+    </task>
+    <task id="LOCAL-00003" summary="Initial commit">
+      <created>1668998479402</created>
+      <option name="number" value="00003" />
+      <option name="presentableId" value="LOCAL-00003" />
+      <option name="project" value="LOCAL" />
+      <updated>1668998479402</updated>
+    </task>
+    <option name="localTasksCounter" value="4" />
     <servers />
   </component>
   <component name="TypeScriptGeneratedFilesManager">
@@ -63,4 +125,8 @@
       </map>
     </option>
   </component>
+  <component name="VcsManagerConfiguration">
+    <MESSAGE value="Initial commit" />
+    <option name="LAST_COMMIT_MESSAGE" value="Initial commit" />
+  </component>
 </project>
\ No newline at end of file
